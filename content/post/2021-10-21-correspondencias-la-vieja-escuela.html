---
title: Análisis de Correspondencias. La vieja escuela
author: jlcr
date: '2021-10-21'
slug: correspondencias-la-vieja-escuela
categories:
  - ciencia de datos
  - estadística
  - R
tags:
  - estadística
  - correspondencias
  - factorial
description: ''
draft: yes
topics: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Quién me conoce sabe que siento debilidad por el análisis de datos categóricos, en particular por técnicas como el análisis de correspondencias simple o múltiple o por las cosas más modernas que hay.
No en vano se me dió especialmente bien en la universidad, en parte debido a que por fin me centré después de unos años locos, y en parte debido a algún buen profesor. El caso es que en el curro utilizamos este tipo de técnicas para encontrar relaciones entre variables categóricas que quizá hayan pasado desapercibidas en un primer análisis.</p>
<p>Antes de nada voy a dar un par de referencias en castellano, bastante útiles.</p>
<ul>
<li><a href="https://www.fbbva.es/wp-content/uploads/2017/05/dat/DE_2008_practica_analisis_correspondencias.pdf">La práctica del análisis de correspondencias. Michael Greenacre</a></li>
<li><a href="https://www.researchgate.net/publication/40944325_Analisis_de_Datos_Multivariantes">Análisis de datos multivariantes. Daniel Peña</a></li>
</ul>
<p>De hecho el ejemplo que voy a contar y la notación que voy a usar viene en el libro de Daniel. Es un ejemplo de Fisher (si, ese al que todo el mundo odia hoy en día) de 1940, sobre la relación entre el color de los ojos (en filas) y el color del pelo (en columnas). Se trata de una simple tabla de contingencia.</p>
<pre class="r"><code>df &lt;- read.csv(&quot;../../data/hair_eyes_color.csv&quot;)

rownames(df) &lt;- df$color_ojos

df_tabla &lt;- as.matrix(df[,-1])

df_tabla</code></pre>
<pre><code>##          rubio pelirrojo castaño oscuro negro
## claros     688       116     584    188     4
## azules     326        38     241    110     3
## castaños   343        84     909    412    26
## oscuros     98        48     403    618    85</code></pre>
<p>Cuyos totales por filas y columnas son</p>
<pre class="r"><code>addmargins(df_tabla)</code></pre>
<pre><code>##          rubio pelirrojo castaño oscuro negro  Sum
## claros     688       116     584    188     4 1580
## azules     326        38     241    110     3  718
## castaños   343        84     909    412    26 1774
## oscuros     98        48     403    618    85 1252
## Sum       1455       286    2137   1328   118 5324</code></pre>
<div id="proyección-de-las-filas." class="section level2">
<h2>Proyección de las filas.</h2>
<p>Podríamos plantearnos la relación entre las filas (color de ojos) , ¿cómo de similares son los que tienen los ojos claros con los que tienen los ojos azules, respecto a su color del pelo?</p>
<p>Parece claro que deberíamos centrarnos en los porcentajes por filas (perfiles fila)</p>
<pre class="r"><code>prop.table(df_tabla, 1)</code></pre>
<pre><code>##               rubio  pelirrojo   castaño    oscuro       negro
## claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646
## azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273
## castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144
## oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374</code></pre>
<p>A partir de ahora vamos a usar la tabla de frecuencias relativas, cuyos elementos llamaremos <span class="math inline">\(f_{ij}\)</span></p>
<pre class="r"><code>(tabla_frecuencias &lt;-  prop.table(df_tabla))</code></pre>
<pre><code>##               rubio   pelirrojo    castaño     oscuro        negro
## claros   0.12922615 0.021788129 0.10969196 0.03531180 0.0007513148
## azules   0.06123216 0.007137491 0.04526672 0.02066116 0.0005634861
## castaños 0.06442524 0.015777611 0.17073629 0.07738542 0.0048835462
## oscuros  0.01840721 0.009015778 0.07569497 0.11607814 0.0159654395</code></pre>
<p>Que tiene los mismos porcentajes por filas.</p>
<pre class="r"><code>(perfiles_fila &lt;- prop.table(tabla_frecuencias, 1))</code></pre>
<pre><code>##               rubio  pelirrojo   castaño    oscuro       negro
## claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646
## azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273
## castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144
## oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374</code></pre>
<p>Se podría considerar utilizar la distancia euclídea para ver como de parecidas son las filas ojos claros y ojos azules, pero eso presenta un problema, que es la distribución del color del pelo en la tabla, que es mayor que la de pelirrojos por ejemplo, así que usar esa distancia no sería justo. Podríamos definir una distancia ponderada que fuera <span class="math inline">\(d(i, i&#39;) = \sum_j ((f_{ij} - f_{i&#39;j})^2/f_{.j})\)</span> dónde <span class="math inline">\(f_{.j}\)</span> es la distribución de las columnas en la población, vamos, qué % de rubios, pelirrojos, etc hay en mis datos.</p>
<p>En forma matricial, en notación de Daniel sería
<strong>insertar captura</strong></p>
<p>Esta distancia es equivalente a la euclídea en la matriz Y definida como dividir cada elemento de los perfiles fila por la raíz cuadrada del peso de la columna.</p>
<pre class="r"><code># peso de loas columnas
(f.j &lt;- colSums(tabla_frecuencias))</code></pre>
<pre><code>##      rubio  pelirrojo    castaño     oscuro      negro 
## 0.27329076 0.05371901 0.40138993 0.24943651 0.02216379</code></pre>
<pre class="r"><code># peso de las filas
(fi. &lt;- rowSums(tabla_frecuencias))</code></pre>
<pre><code>##    claros    azules  castaños   oscuros 
## 0.2967693 0.1348610 0.3332081 0.2351615</code></pre>
<p>Para hacerlo lo hacemos usando matrices</p>
<pre class="r"><code># matriz diagonal con la raíz de los porcentjes totales de las columnas( col masses)
(DC_12 &lt;- diag(1/sqrt(f.j)))</code></pre>
<pre><code>##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,] 1.912879 0.000000 0.000000 0.000000 0.000000
## [2,] 0.000000 4.314555 0.000000 0.000000 0.000000
## [3,] 0.000000 0.000000 1.578399 0.000000 0.000000
## [4,] 0.000000 0.000000 0.000000 2.002258 0.000000
## [5,] 0.000000 0.000000 0.000000 0.000000 6.717041</code></pre>
<pre class="r"><code># Matriz diagonal con 1/fi. 
(DF_1 &lt;- diag(1/fi.))</code></pre>
<pre><code>##         [,1]     [,2]     [,3]     [,4]
## [1,] 3.36962 0.000000 0.000000 0.000000
## [2,] 0.00000 7.415042 0.000000 0.000000
## [3,] 0.00000 0.000000 3.001127 0.000000
## [4,] 0.00000 0.000000 0.000000 4.252396</code></pre>
<p>Matriz Y</p>
<pre class="r"><code># se puede hacer usando los perfiles fila o directamente utilizando DF_1 y DC_12
# Y &lt;- perfiles_fila %*% DC_12

Y &lt;- DF_1 %*% as.matrix(tabla_frecuencias) %*% DC_12
rownames(Y) &lt;- rownames(tabla_frecuencias)
colnames(Y) &lt;- colnames(tabla_frecuencias)
Y</code></pre>
<pre><code>##              rubio pelirrojo   castaño    oscuro      negro
## claros   0.8329499 0.3167648 0.5834082 0.2382433 0.01700517
## azules   0.8685217 0.2283469 0.5297968 0.3067526 0.02806563
## castaños 0.3698521 0.2042969 0.8087737 0.4650114 0.09844593
## oscuros  0.1497302 0.1654142 0.5080629 0.9883349 0.45602916</code></pre>
<p>En esta tabla Y, la distancia euclídea entre filas coincide con la distancia ponderada que habíamos definido dónde la distancia entre dos filas venía ponderada por el peso de cada columna.</p>
<p>Podríamos ahora plantearnos utilizar una descomposición en valores y vectores propios sobre esta tabla, pero tendríamos el problema de que el peso de cada fila sería el mismo, por eso se hace necesario tener en cuenta el peso de cada fila.</p>
<p>Podemos construir ahora una matriz <strong>Z</strong> dónde se pondere por el peso de las filas y el de las columnas.</p>
<pre class="r"><code># cremaos matriz diagonal con 1/sqrt(fi.)
(DF_12 &lt;- diag(1 / sqrt(fi.)))</code></pre>
<pre><code>##          [,1]     [,2]     [,3]     [,4]
## [1,] 1.835653 0.000000 0.000000 0.000000
## [2,] 0.000000 2.723057 0.000000 0.000000
## [3,] 0.000000 0.000000 1.732376 0.000000
## [4,] 0.000000 0.000000 0.000000 2.062134</code></pre>
<p>Creamos matriz Z como</p>
<pre class="r"><code>Z &lt;- DF_12 %*% as.matrix(tabla_frecuencias) %*% DC_12

rownames(Z) &lt;- rownames(tabla_frecuencias)
colnames(Z) &lt;- colnames(tabla_frecuencias)
Z</code></pre>
<pre><code>##               rubio  pelirrojo   castaño    oscuro       negro
## claros   0.45376229 0.17256250 0.3178206 0.1297867 0.009263827
## azules   0.31895094 0.08385681 0.1945596 0.1126501 0.010306662
## castaños 0.21349407 0.11792869 0.4668580 0.2684240 0.056827106
## oscuros  0.07260933 0.08021509 0.2463773 0.4792778 0.221144304</code></pre>
<p>Sobre esta matriz Z que no es más que la tabla de frecuencias relativas estandarizada por el peso de las filas y el de las columnas podemos diagonalizar la matriz <strong>Z’Z</strong> Esta matriz tiene un valor propio igual a 1, pero los importantes son los siguientes.</p>
<pre class="r"><code>res_diag &lt;- eigen(t(Z) %*% Z)
res_diag$values</code></pre>
<pre><code>## [1]  1.000000e+00  1.873957e-01  2.847581e-02  9.026139e-04 -5.424744e-18</code></pre>
<pre class="r"><code>res_diag$vectors</code></pre>
<pre><code>##            [,1]        [,2]        [,3]        [,4]        [,5]
## [1,] -0.5227722 -0.64488569  0.50331668 -0.21984084  0.09578107
## [2,] -0.2317736 -0.11671207  0.06710395  0.91248080 -0.30908759
## [3,] -0.6335534 -0.02776729 -0.74923836 -0.04663792  0.18521831
## [4,] -0.4994362  0.65005451  0.30376531 -0.21371886 -0.43593979
## [5,] -0.1488751  0.38361288  0.29755318  0.26682945  0.81911020</code></pre>
<p>Teniendo la matriz Y, que son los perfiles (porcentajes) fila ponderados por el peso de las columnas, podemos proyectar esas filas sobre las dimensiones obtenidas por los vectores propios obtenidos asociados a los autovalores menores que 1. Esa será la mejor representación de las filas en un subespacio de las columnas.</p>
<pre class="r"><code># proyeccion 1. con valor menor que 1 
vector_propio1 &lt;- res_diag$vectors[,2]
vector_propio2 &lt;- res_diag$vectors[,3]

(coord1 &lt;-Y %*% vector_propio1 )</code></pre>
<pre><code>##                 [,1]
## claros   -0.42893286
## azules   -0.39128686
## castaños  0.05523421
## oscuros   0.68743802</code></pre>
<pre class="r"><code>(coord2 &lt;-Y %*% vector_propio2 )</code></pre>
<pre><code>##                 [,1]
## claros    0.08081194
## azules    0.15705214
## castaños -0.23555524
## oscuros   0.14171621</code></pre>
<p>Y ya lo puedo pintar</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
## ✓ tibble  3.1.5     ✓ dplyr   1.0.7
## ✓ tidyr   1.1.4     ✓ stringr 1.4.0
## ✓ readr   2.0.2     ✓ forcats 0.5.1</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>(to_plot &lt;-  data.frame(Dim1 = coord1[,1], Dim2 = coord2[,1], color_ojos= rownames(Y)))</code></pre>
<pre><code>##                 Dim1        Dim2 color_ojos
## claros   -0.42893286  0.08081194     claros
## azules   -0.39128686  0.15705214     azules
## castaños  0.05523421 -0.23555524   castaños
## oscuros   0.68743802  0.14171621    oscuros</code></pre>
<pre class="r"><code>to_plot %&gt;% 
  ggplot(aes(x=Dim1, y=Dim2)) +
  geom_label(aes(label= color_ojos)) +
  scale_x_continuous(limits = c(-0.8,0.8))</code></pre>
<p><img src="/post/2021-10-21-correspondencias-la-vieja-escuela_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="proyección-de-las-columnas" class="section level2">
<h2>Proyección de las columnas</h2>
<p>La proyección de las columnas en un subespacio de las filas se hace de manera análoga , solo que en vez de diagonalizar <strong>Z’Z</strong> se hace con <strong>Z Z’</strong>, que tiene los mismos valores propios que los obtenidos. De aquí viene la relación baricéntrica entre las filas y las columnas.</p>
</div>
<div id="relación-con-la-distancia-chi-cuadrado" class="section level2">
<h2>Relación con la distancia Chi-cuadrado</h2>
<p>Si se desarrolla la expresión de la distancia Chi-cuadrado , tal como se hace en el libro de Daniel Peña se llega a que se corresponde con la distancia euclídea en la matriz <strong>Y</strong> que son los perfiles filas ponderados por el peso de cada columna. Esta implicación es importante, puesto que al descomponer el estadístico Chi-cuadrado que nos mide la asociación entre variables categóricas (filas y columnas), estamos descubriendo qué filas están asociados con determinadas columnas.</p>
</div>
<div id="uso-con-factominer" class="section level2">
<h2>Uso con FactoMineR</h2>
<p>Sólo trataba de dar una pequeña explicación de la relación entre el análisis de correspondencias y la diagonalización de matrices. Hay mucha más explicación en los libros que he enlazado al principio. En el día a día, podemos usar librerías específicas para calcular este análisis, como <code>FactoMineR</code> en R o <code>prince</code> en python.</p>
<p>Veamos como se usa con <code>FactoMiner</code></p>
<pre class="r"><code>library(FactoMineR)
library(factoextra) # pa los dibujitos</code></pre>
<pre><code>## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa</code></pre>
<pre class="r"><code>res_ca &lt;- CA(df_tabla, graph = FALSE)</code></pre>
<p>Y podemos pintar las filas en el espacio de las columnas</p>
<pre class="r"><code>fviz_ca_row(res_ca) +
   scale_x_continuous(limits = c(-0.8,0.8))</code></pre>
<p><img src="/post/2021-10-21-correspondencias-la-vieja-escuela_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Y vemos que las coordenadas son las mismas que hemos obtenido nosotros antes</p>
<pre class="r"><code>res_ca$row$coord</code></pre>
<pre><code>##                Dim 1       Dim 2        Dim 3
## claros   -0.42893286  0.08081194  0.032336831
## azules   -0.39128686  0.15705214 -0.065353061
## castaños  0.05523421 -0.23555524 -0.005724586
## oscuros   0.68743802  0.14171621  0.004781725</code></pre>
<pre class="r"><code>to_plot</code></pre>
<pre><code>##                 Dim1        Dim2 color_ojos
## claros   -0.42893286  0.08081194     claros
## azules   -0.39128686  0.15705214     azules
## castaños  0.05523421 -0.23555524   castaños
## oscuros   0.68743802  0.14171621    oscuros</code></pre>
<p>La representación conjunta.</p>
<pre class="r"><code>fviz_ca(res_ca) </code></pre>
<p><img src="/post/2021-10-21-correspondencias-la-vieja-escuela_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>La librería <code>FactoMineR</code> junto con <code>factoextra</code> devuelven también múltiples ayudas a la interpretación como la contribución de cada fila o columna a la estructura factorial etc.
Por otro lado, la librería <code>FactoInvestigate</code> que toma como input un análisis factorial (pca, ca o mca), devuelve un informe en inglés (en Rmd) describiendo lo que significa cada dimensión obtenida.</p>
</div>
