---
title: 'Palabras para Julia ( Parte 1/n) '
author: jlcr
date: '2021-08-07'
slug: palabras-para-julia-parte-1-n
categories:
  - Julia
  - ciencia de datos
  - software
tags: 
  - Julia
  - Lenguajes de programaciÃ³n
description: ''
topics: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>A pesar del tÃ­tulo, no voy a hablar sobre la excelente canciÃ³n de <a href="https://youtu.be/WowU0UKaPyE">los Suaves</a>, sino del lenguaje de programaciÃ³n <a href="https://julialang.org/">Julia</a>. Ya en otra entrada del <a href="https://muestrear-no-es-pecado.netlify.app/2019/03/06/glmer-vs-julia-vs-inla/">blog</a> de hace un par de aÃ±os comparÃ© <code>glmer</code> con <code>INLA</code> y la librerÃ­a <code>MixedModels</code>. Por aquel entonces la versiÃ³n de Julia era la 1.0.3, ya va por la 1.6.2. Debido a reciente entrada de <a href="https://www.datanalytics.com/2021/07/14/mi-apuesta-para-el-larguisimo-plazo-julia/">Carlos</a> dÃ³nde apostaba por Julia para el larguÃ­simo plazo, he decidido echarle un vistazo un poco mÃ¡s en profundidad.</p>
<p>Lo cierto es que me estÃ¡ gustando bastante el lenguaje y voy a escribir un par de entradas dÃ³nde contar alguna cosilla. Ya Carlos mencionaba que Julia corre sobre <a href="https://en.wikipedia.org/wiki/LLVM">LLVM</a>, pero tambiÃ©n cabe mencionar que Julia tiene caracterÃ­sticas mÃ¡s que interesantes, como multiple dispatch o tipos abstractos que permiten al desarrollador escribir cÃ³digo sin preocuparse demasiado por el tipado y que sea el compilador el que cree los mÃ©todos especÃ­ficos. Si, has oÃ­do bien, Julia compila las funciones, por lo que tiene la doble ventaja de ser un lenguaje rÃ¡pido a la vez que sencillo, bueno, su lema dice â€œTan fÃ¡cil como Python, tan rÃ¡pido como Câ€.</p>
<p>En esta primera entrada voy a poner un ejemplo sencillo de cÃ³mo serÃ­a hacer un modelo de â€œMachÃ­n Leninâ€ utilizando la librerÃ­a <a href="https://github.com/alan-turing-institute/MLJ.jl"><code>MLJ</code></a>, y en el post siguiente os contarÃ© como tener un binario para predecir usando ese modelo de forma que funcione en cualquier Linux sin importar si estÃ¡ basado en <code>Debian</code>, <code>Centos</code> o lo que sea, y sin necesidad de tener instalado Julia, ni docker, ni nada.</p>
<div id="modelo-con-mlj" class="section level2">
<h2>Modelo con MLJ</h2>
<p>MLJ es una librerÃ­a que pretende servir de interfaz comÃºn a otras muchas librerÃ­as. Veamos un ejemplo de como ajustar un <code>RandomForest</code> implementado en la librerÃ­a <code>DecisionTree</code>.</p>
<p>Lo primero, para instalar paquetes podÃ©is mirar <a href="http://pkgdocs.julialang.org/latest/getting-started/">esto</a>, bÃ¡sicamente haces</p>
<pre class="julia"><code>using Pkg
Pkg.import(&quot;nombre_paquete&quot;)
</code></pre>
<p>O en el REPL de Julia entras en el modo Package pulsando <code>]</code> y pones <code>add nombre_paquete</code> . Esto bajarÃ¡ la librerÃ­a correspondiente precompilado y la aÃ±ade a <code>~/.julia/packages/</code></p>
<p>Vamos al ejemplo. Aunque voy a usar chunks de julia (gracias a la librerÃ­a JuliaCall) en el rmarkdown dÃ³nde escribo los posts, en realidad como editor par Julia me gusta VSCode.</p>
<p>Los datos de ejemplo son de la librerÃ­a <code>boot</code>de R . puedes ver la ayuda haciendo en R</p>
<pre class="r"><code>library(boot)
help(channing)</code></pre>
<pre><code>Channing House Data
Description
The channing data frame has 462 rows and 5 columns.

Channing House is a retirement centre in Palo Alto, California. These data were collected between the opening of the house in 1964 until July 1, 1975. In that time 97 men and 365 women passed through the centre. For each of these, their age on entry and also on leaving or death was recorded. A large number of the observations were censored mainly due to the resident being alive on July 1, 1975 when the data was collected. Over the time of the study 130 women and 46 men died at Channing House. Differences between the survival of the sexes, taking age into account, was one of the primary concerns of this study.

Usage
channing
Format
This data frame contains the following columns:

sex
A factor for the sex of each resident (&quot;Male&quot; or &quot;Female&quot;).

entry
The residents age (in months) on entry to the centre

exit
The age (in months) of the resident on death, leaving the centre or July 1, 1975 whichever event occurred first.

time
The length of time (in months) that the resident spent at Channing House. (time=exit-entry)

cens
The indicator of right censoring. 1 indicates that the resident died at Channing House, 0 indicates that they left the house prior to July 1, 1975 or that they were still alive and living in the centre at that date.
</code></pre>
<p>En Julia podemos instalar la librerÃ­a <code>RDatasets</code> y usar esos datos</p>
<pre class="julia"><code>using RDatasets, MLJ
channing = dataset(&quot;boot&quot;, &quot;channing&quot;)</code></pre>
<pre><code>## 462Ã—5 DataFrame
##  Row â”‚ Sex     Entry  Exit   Time   Cens
##      â”‚ Catâ€¦    Int32  Int32  Int32  Int32
## â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##    1 â”‚ Male      782    909    127      1
##    2 â”‚ Male     1020   1128    108      1
##    3 â”‚ Male      856    969    113      1
##    4 â”‚ Male      915    957     42      1
##    5 â”‚ Male      863    983    120      1
##    6 â”‚ Male      906   1012    106      1
##    7 â”‚ Male      955   1055    100      1
##    8 â”‚ Male      943   1025     82      1
##   â‹®  â”‚   â‹®       â‹®      â‹®      â‹®      â‹®
##  456 â”‚ Female    986   1030     44      1
##  457 â”‚ Female   1039   1132     93      1
##  458 â”‚ Female    968    990     22      1
##  459 â”‚ Female    955    990     35      1
##  460 â”‚ Female    837    911     74      1
##  461 â”‚ Female    861    915     54      1
##  462 â”‚ Female    967    983     16      1
##                           447 rows omitted</code></pre>
<p>MLJ necesita que las columnas tenga los tipos correctos en <a href="https://github.com/JuliaAI/ScientificTypes.jl"><code>scitypes</code></a>. Podemos verlos con</p>
<pre class="julia"><code>schema(channing)</code></pre>
<pre><code>## â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
## â”‚ _.names â”‚ _.types                         â”‚ _.scitypes    â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ Sex     â”‚ CategoricalValue{String, UInt8} â”‚ Multiclass{2} â”‚
## â”‚ Entry   â”‚ Int32                           â”‚ Count         â”‚
## â”‚ Exit    â”‚ Int32                           â”‚ Count         â”‚
## â”‚ Time    â”‚ Int32                           â”‚ Count         â”‚
## â”‚ Cens    â”‚ Int32                           â”‚ Count         â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
## _.nrows = 462</code></pre>
<p>Queremos modelar la variable <code>exit</code>. <code>MLJ</code> quiere la <code>y</code> por un lado y las Xâ€™s por otro, para eso vamos a usar la funciÃ³n <code>unpack</code> que ademÃ¡s de permitir eso permite cambiar el tipo de las variables, y convertir la variable Cens a categÃ³rica por ejemplo</p>
<pre class="julia"><code>y, X =  unpack(channing,
                      ==(:Exit),            # con el doble igual seleccionamos la y
                      !=(:Time);            # Quitamos variable Time
                      :Exit=&gt;Continuous,    # Convertimos al tipo correcto en scitypes
                      :Entry=&gt;Continuous,
                      :Cens=&gt;Multiclass)</code></pre>
<pre><code>## ([909.0, 1128.0, 969.0, 957.0, 983.0, 1012.0, 1055.0, 1025.0, 1043.0, 945.0  â€¦  905.0, 1040.0, 926.0, 1030.0, 1132.0, 990.0, 990.0, 911.0, 915.0, 983.0], 462Ã—3 DataFrame
##  Row â”‚ Sex     Entry    Cens
##      â”‚ Catâ€¦    Float64  Catâ€¦
## â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##    1 â”‚ Male      782.0  1
##    2 â”‚ Male     1020.0  1
##    3 â”‚ Male      856.0  1
##    4 â”‚ Male      915.0  1
##    5 â”‚ Male      863.0  1
##    6 â”‚ Male      906.0  1
##    7 â”‚ Male      955.0  1
##    8 â”‚ Male      943.0  1
##   â‹®  â”‚   â‹®        â‹®      â‹®
##  456 â”‚ Female    986.0  1
##  457 â”‚ Female   1039.0  1
##  458 â”‚ Female    968.0  1
##  459 â”‚ Female    955.0  1
##  460 â”‚ Female    837.0  1
##  461 â”‚ Female    861.0  1
##  462 â”‚ Female    967.0  1
##              447 rows omitted)</code></pre>
<p>Ahora ya podemos ver como se hace el modelo.</p>
<pre class="julia"><code>Tree = @load RandomForestRegressor pkg=DecisionTree</code></pre>
<pre><code>## import MLJDecisionTreeInterface âœ”</code></pre>
<pre><code>## MLJDecisionTreeInterface.RandomForestRegressor</code></pre>
<pre class="julia"><code>tree = Tree(n_trees = 20) # tambien se puede instanciar sin paraÃ¡metros </code></pre>
<pre><code>## RandomForestRegressor(
##     max_depth = -1,
##     min_samples_leaf = 1,
##     min_samples_split = 2,
##     min_purity_increase = 0.0,
##     n_subfeatures = -1,
##     n_trees = 20,
##     sampling_fraction = 0.7,
##     pdf_smoothing = 0.0)[34m @867[39m</code></pre>
<pre class="julia"><code># y usar tree.n_trees = 20
</code></pre>
<p>Como tenemos la variable <code>Cens</code>que es categÃ³rica necesitamos codificarla, aquÃ­ entra como hacer un pipeline en MLJ, que es una de las cosas mÃ¡s potentes que tiene junto con los <a href="https://alan-turing-institute.github.io/MLJ.jl/stable/composing_models/">ComposingModels</a> models que permite mezclar varios modelos.</p>
<pre class="julia"><code># Definimos un ContinuosEncoder, ver la ayuda con ?ContinousEncoder en el repl de julia

  ContinuousEncoder(one_hot_ordered_factors=false, drop_last=false)</code></pre>
<pre><code>## ContinuousEncoder(
##     drop_last = false,
##     one_hot_ordered_factors = false)[34m @541[39m</code></pre>
<pre class="julia"><code>
  # Unsupervised model for arranging all features (columns) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:
  # 
  #   â€¢  If ftr is already Continuous retain it.
  # 
  #   â€¢  If ftr is Multiclass, one-hot encode it.
  # 
  #   â€¢  If ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case
  #      one-hot encode it.
  # 
  #   â€¢  If ftr is Count, replace it with coerce(ftr, Continuous).
  # 
  #   â€¢  If ftr is of some other element scitype, or was not observed in fitting the encoder, drop it from the table.
  # 
  # If drop_last=true is specified, then one-hot encoding always drops the last class indicator colum

hot = ContinuousEncoder(one_hot_ordered_factors=true, drop_last=true)</code></pre>
<pre><code>## ContinuousEncoder(
##     drop_last = true,
##     one_hot_ordered_factors = true)[34m @184[39m</code></pre>
<p>Utilizamos la macro <code>@pipeline</code> para encadenar el onehot y el modelo</p>
<pre class="julia"><code>pipe = @pipeline hot tree</code></pre>
<pre><code>## Pipeline259(
##     continuous_encoder = ContinuousEncoder(
##             drop_last = true,
##             one_hot_ordered_factors = true),
##     random_forest_regressor = RandomForestRegressor(
##             max_depth = -1,
##             min_samples_leaf = 1,
##             min_samples_split = 2,
##             min_purity_increase = 0.0,
##             n_subfeatures = -1,
##             n_trees = 20,
##             sampling_fraction = 0.7,
##             pdf_smoothing = 0.0))[34m @513[39m</code></pre>
<p>Y ya podemos ajustar el modelo por ejemplo utilizando <code>evaluate</code> y validaciÃ³n cruzada</p>
<pre class="julia"><code>evaluate(pipe, X, y, resampling=CV(nfolds=5), measure = [rmse, mae])</code></pre>
<pre><code>## â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
## â”‚ _.measure                 â”‚ _.measurement â”‚ _.per_fold                     â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ RootMeanSquaredError @956 â”‚ 55.6          â”‚ [48.1, 62.0, 53.8, 50.5, 61.9] â”‚
## â”‚ MeanAbsoluteError @225    â”‚ 46.6          â”‚ [40.9, 53.1, 43.8, 42.0, 53.4] â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
## _.per_observation = [missing, missing]
## _.fitted_params_per_fold = [ â€¦ ]
## _.report_per_fold = [ â€¦ ]</code></pre>
<p>Hay mÃ¡s medidas que se pueden listar con <code>measures()</code></p>
<p>TambiÃ©n podemos partir en train, test y similar</p>
<pre class="julia"><code>train, test = partition(eachindex(y), 0.7, shuffle=true, rng=155);</code></pre>
<p>Ahora instanciamos el modelo con <code>machine</code> especificando la X, y la y</p>
<pre class="julia"><code>modelo = machine(pipe, X,y)</code></pre>
<pre><code>## [34mMachine{Pipeline259,â€¦} @980[39m trained 0 times; caches data
##   args: 
##     1:   [34mSource @265[39m â `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`
##     2:   [34mSource @897[39m â `AbstractVector{Continuous}`</code></pre>
<p>Y podemos usar <code>fit!</code> para ajustar â€œin placeâ€ (en julia si la funciÃ³n acaba en ! es una funciÃ³n que actua modificando el objeto que se le pasa) sin tener que crear otra variable</p>
<pre class="julia"><code>
fit!(modelo, rows = train)</code></pre>
<pre><code>## [34mMachine{Pipeline259,â€¦} @980[39m trained 1 time; caches data
##   args: 
##     1:   [34mSource @265[39m â `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`
##     2:   [34mSource @897[39m â `AbstractVector{Continuous}`</code></pre>
<p>Y ya podrÃ­amos predecir sobre test, dÃ³nde se le aplicarÃ­a el onehot encoder que hemos definido en el pipeline</p>
<pre class="julia"><code># En julia podemos usar sintaxis latex por ejemplo \beta\hat  y tabulador despues de beta y hat en vscode 
# escribe Î²Ì‚ (uso juliaMono https://juliamono.netlify.app/) como tipo de letra 
yÌ‚ = predict(modelo,X[test, :])</code></pre>
<pre><code>## 139-element Vector{Float64}:
##  1068.7
##  1069.05
##  1025.6166666666666
##  1025.85
##   933.4083333333332
##   956.5
##  1119.25
##   968.6666666666666
##   949.1375
##   866.4625
##     â‹®
##   883.2175
##   867.725
##  1021.2833333333332
##   936.0666666666668
##   938.7166666666668
##   929.85
##  1077.9333333333334
##   945.1625
##   944.9625</code></pre>
<p>En este caso hemos hecho un modelo para predecir una variable continua, cuando sea categÃ³rica existen funciones que devuelven la clase predicha o la probabilidad de cada clase.</p>
<p>TambiÃ©n podemos ver como varÃ­a el error segÃºn el nÃºmero de Ã¡rboles</p>
<pre class="julia"><code>modelo</code></pre>
<pre><code>## [34mMachine{Pipeline259,â€¦} @980[39m trained 1 time; caches data
##   args: 
##     1:   [34mSource @265[39m â `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`
##     2:   [34mSource @897[39m â `AbstractVector{Continuous}`</code></pre>
<pre class="julia"><code>r_tree = range(pipe, :(random_forest_regressor.n_trees), lower=2, upper=20)</code></pre>
<pre><code>## typename(MLJBase.NumericRange)(Int64, :(random_forest_regressor.n_trees), ... )</code></pre>
<pre class="julia"><code>
curve = MLJ.learning_curve(modelo;
                           range=r_tree,
                           resampling=CV(nfolds=5),
                           measure=rmse)</code></pre>
<pre><code>## (parameter_name = &quot;random_forest_regressor.n_trees&quot;,
##  parameter_scale = :linear,
##  parameter_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
##  measurements = [65.42183246458882, 59.11119640237987, 55.7512163366967, 58.34159065560261, 57.134134628248496, 56.97838965317687, 56.68078029656279, 56.19162995151205, 57.57519156288071, 55.922943397008744, 55.16193489943914, 55.83221711509146, 56.205393291313506, 56.830588361395094, 56.49114184998105, 55.58539209834001, 56.418841059687026, 56.17205183553797, 56.171363302332196],)</code></pre>
<pre class="julia"><code>using Plots
gr() # especificamos un </code></pre>
<pre><code>## Plots.GRBackend()</code></pre>
<pre class="julia"><code>plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;CV estimate of RMSE error&quot;)</code></pre>
<p><img src="/post/2021-08-07-palabras-para-julia-parte-1-n_files/figure-html/unnamed-chunk-16-J1.png" width="300" />
Y esto es todo, en el prÃ³ximo post contarÃ© como crear el binario que nos va a permitir tener un motor de predicciÃ³n para los modelos de un Ã¡rbol de decisiÃ³n y que funcione en cualquier linux. Casi listo para producciÃ³n (o al menos una parte importante) sin tener que tener julia en dÃ³nde se vaya a utilizar.</p>
</div>
